import os
import numpy as np
import json
import gymnasium as gym
import random

import sumo_gym
from sumo_gym.utils.fmp_utils import (
    Vertex,
    Edge,
    Demand,
    DeliveryTruck,
    TimeManager,
    generate_random_time_windows,
    get_available_deliveries,
)
from DQN.dqn import (
    QNetwork,
    ReplayBuffer,
    run_target_update,
)
from statistics import mean

suffix = "-vrptw.json"

from sumo_gym.envs.fmp import VRPTWEnv

# ì‹œê°í™”ìš© í™˜ê²½ ìƒì„± (GUI ì¼œê¸°)
vis_env = VRPTWEnv(
    mode="sumo_config",
    verbose=1,
    sumo_config_path="../assets/data/cosmos/cosmos.sumocfg",
    net_xml_file_path="../assets/data/cosmos/cosmos.net.xml",
    demand_xml_file_path="../assets/data/cosmos/cosmos.rou.xml",
    additional_xml_file_path="../assets/data/cosmos/cosmos.cs.add.xml",
    render_env=True,  # GUI ì¼œê¸°
)

class VRPTWVisualizer(object):
    def __init__(self, env):
        self.env = env
        
        # í•™ìŠµëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°ìš© ë„¤íŠ¸ì›Œí¬ ìƒì„±
        state_size = 105  # í˜„ì¬ì‹œê°„(1) + ìœ„ì¹˜(1) + ì ì¬ëŸ‰(1) + ë°°ì†¡ìƒíƒœ(50) + ì‹œê°„ì°½ì •ë³´(50Ã—2)
        action_size = 52  # í—ˆë¸Œ(1) + ë°°ì†¡ì§€(50) + ëŒ€ê¸°(1)
        
        self.q_network = QNetwork(state_size, action_size, 0.003)
        self.time_manager = TimeManager(max_episode_time=180)  # 3ì‹œê°„
        
        # í•™ìŠµëœ ê°€ì¤‘ì¹˜ê°€ ìˆë‹¤ë©´ ë¶ˆëŸ¬ì˜¤ê¸° (ì„ íƒì‚¬í•­)
        # self.load_trained_weights()
    
    def load_trained_weights(self):
        """í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¶ˆëŸ¬ì˜¤ê¸° (êµ¬í˜„ í•„ìš”)"""
        # ì˜ˆì‹œ: 
        # try:
        #     self.q_network.load_state_dict(torch.load("vrptw_model.pth"))
        #     print("âœ… í•™ìŠµëœ ëª¨ë¸ì„ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤!")
        # except:
        #     print("âš ï¸ í•™ìŠµëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íœ´ë¦¬ìŠ¤í‹±ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        pass
    
    def _get_state_vector(self, agent):
        """í˜„ì¬ ìƒíƒœë¥¼ ë²¡í„°ë¡œ ë³€í™˜"""
        agent_idx = self.env.agent_name_idx_mapping[agent]
        truck = self.env.vrptw.delivery_trucks[agent_idx]
        
        # ê¸°ë³¸ ìƒíƒœ ì •ë³´
        state = [
            truck.current_time / 180.0,  # ì •ê·œí™”ëœ í˜„ì¬ ì‹œê°„
            truck.location / 50.0,       # ì •ê·œí™”ëœ ìœ„ì¹˜
            truck.cargo_count / 5.0,     # ì •ê·œí™”ëœ ì ì¬ëŸ‰
        ]
        
        # ë°°ì†¡ ì™„ë£Œ ìƒíƒœ (50ê°œ)
        delivery_status = [1.0 if demand.is_completed else 0.0 
                          for demand in self.env.vrptw.demands]
        state.extend(delivery_status)
        
        # ì‹œê°„ì°½ ì •ë³´ (50ê°œ Ã— 2)
        for demand in self.env.vrptw.demands:
            state.extend([
                demand.earliest_time / 180.0,  # ì •ê·œí™”ëœ ì‹œì‘ ì‹œê°„
                demand.latest_time / 180.0,    # ì •ê·œí™”ëœ ì¢…ë£Œ ì‹œê°„
            ])
        
        return np.array(state, dtype=np.float32)

    def _get_available_actions(self, agent):
        """í˜„ì¬ ìƒíƒœì—ì„œ ê°€ëŠ¥í•œ ì•¡ì…˜ ëª©ë¡"""
        agent_idx = self.env.agent_name_idx_mapping[agent]
        truck = self.env.vrptw.delivery_trucks[agent_idx]
        available_actions = []
        
        # í—ˆë¸Œ ë³µê·€ (í•­ìƒ ê°€ëŠ¥)
        available_actions.append(0)
        
        # ë°°ì†¡ì§€ ì´ë™ (ì ì¬ëŸ‰ì´ ìˆê³  ì™„ë£Œë˜ì§€ ì•Šì€ ë°°ì†¡ì§€)
        if truck.cargo_count > 0:
            for i, demand in enumerate(self.env.vrptw.demands):
                if not demand.is_completed:
                    # ì‹œê°„ ì œì•½ í™•ì¸
                    travel_time = self.time_manager.calculate_travel_time(
                        self.env.vrptw.vertices, self.env.vrptw.edges,
                        truck.location, demand.destination
                    )
                    arrival_time = truck.current_time + travel_time
                    
                    if arrival_time <= self.time_manager.max_episode_time:
                        available_actions.append(i + 1)  # ë°°ì†¡ì§€ ì•¡ì…˜
        
        # ëŒ€ê¸° (í•­ìƒ ê°€ëŠ¥)
        available_actions.append(51)
        
        return available_actions

    def _select_action(self, agent):
        """ì•¡ì…˜ ì„ íƒ (í•™ìŠµëœ ëª¨ë¸ ë˜ëŠ” íœ´ë¦¬ìŠ¤í‹±)"""
        state = self._get_state_vector(agent)
        available_actions = self._get_available_actions(agent)
        
        if not available_actions:
            return 51  # ëŒ€ê¸°
        
        # í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš© ì‹œë„
        try:
            q_values = self.q_network.compute_q_values(state)
            # ê°€ëŠ¥í•œ ì•¡ì…˜ ì¤‘ì—ì„œ ìµœê³  Qê°’ ì„ íƒ
            available_q_values = [(action, q_values[action]) for action in available_actions]
            return max(available_q_values, key=lambda x: x[1])[0]
        except:
            # ëª¨ë¸ì´ ì—†ê±°ë‚˜ ì˜¤ë¥˜ê°€ ìˆìœ¼ë©´ íœ´ë¦¬ìŠ¤í‹± ì‚¬ìš©
            return self._heuristic_action(agent, available_actions)
    
    def _heuristic_action(self, agent, available_actions):
        """íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ ì•¡ì…˜ ì„ íƒ"""
        agent_idx = self.env.agent_name_idx_mapping[agent]
        truck = self.env.vrptw.delivery_trucks[agent_idx]
        
        # ìš°ì„ ìˆœìœ„ ê¸°ë°˜ íœ´ë¦¬ìŠ¤í‹±
        # 1. ì‹œê°„ì´ ë§ì´ ì§€ë‚¬ìœ¼ë©´ í—ˆë¸Œë¡œ ë³µê·€
        if truck.current_time > 150:
            return 0
        
        # 2. ì ì¬ëŸ‰ì´ ìˆìœ¼ë©´ ë°°ì†¡ ìš°ì„ 
        if truck.cargo_count > 0:
            delivery_actions = [a for a in available_actions if 1 <= a <= 50]
            if delivery_actions:
                # ì‹œê°„ì°½ì´ ê°€ì¥ ê¸‰í•œ ë°°ì†¡ì§€ ìš°ì„ 
                urgent_deliveries = []
                for action in delivery_actions:
                    demand_idx = action - 1
                    demand = self.env.vrptw.demands[demand_idx]
                    urgency = demand.latest_time - truck.current_time
                    urgent_deliveries.append((action, urgency))
                
                # ê°€ì¥ ê¸´ê¸‰í•œ ë°°ì†¡ì§€ ì„ íƒ
                return min(urgent_deliveries, key=lambda x: x[1])[0]
        
        # 3. ê¸°ë³¸ì ìœ¼ë¡œ í—ˆë¸Œë¡œ ë³µê·€ (í™”ë¬¼ ì ì¬)
        return 0
    
    def run_episode(self):
        """í•™ìŠµëœ ëª¨ë¸ë¡œ 1 ì—í”¼ì†Œë“œ ì‹¤í–‰"""
        print("="*70)
        print("ğŸšš VRPTW Fleet Management Visualization ğŸšš")
        print("="*70)
        print("ğŸ“º SUMO GUI Controls:")
        print("   - Space: Start/Pause simulation")
        print("   - +/-: Speed up/down")
        print("   - Ctrl+A: Maximum speed")
        print("   - View menu: Customize display options")
        print("="*70)
        
        self.env.reset()
        episode_step = 0
        completed_deliveries = 0
        truck_stats = {agent: {"distance": 0, "deliveries": 0, "time_violations": 0} 
                      for agent in self.env.possible_agents}
        
        print(f"ğŸ¯ Mission: Deliver to 50 locations using 5 trucks")
        print(f"â° Time limit: 3 hours (180 minutes)")
        print("-" * 70)
        
        for agent in self.env.agent_iter():
            agent_idx = self.env.agent_name_idx_mapping[agent]
            truck = self.env.vrptw.delivery_trucks[agent_idx]
            
            print(f"Step {episode_step:3d}: {agent}")
            print(f"  ğŸ“ Location: {truck.location:2d}, "
                  f"â° Time: {truck.current_time:5.1f}min, "
                  f"ğŸ“¦ Cargo: {truck.cargo_count}")
            
            # ì•¡ì…˜ ì„ íƒ
            action = self._select_action(agent)
            
            # ì•¡ì…˜ ì´ë¦„ ë° ìƒì„¸ ì •ë³´
            if action == 0:
                action_name = "ğŸ  RETURN_TO_HUB"
                action_detail = "í™”ë¬¼ ì ì¬"
            elif 1 <= action <= 50:
                demand_idx = action - 1
                demand = self.env.vrptw.demands[demand_idx]
                action_name = f"ğŸ“¦ DELIVER_TO_{action:02d}"
                action_detail = f"(ì‹œê°„ì°½: {demand.earliest_time:.0f}-{demand.latest_time:.0f})"
            else:
                action_name = "â³ WAIT"
                action_detail = "ëŒ€ê¸°"
            
            print(f"  ğŸ¯ Action: {action_name} {action_detail}")
            
            # í™˜ê²½ì—ì„œ ì•¡ì…˜ ì‹¤í–‰
            observation, reward, done, info = self.env.last()
            self.env.step(action)
            
            episode_step += 1
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            new_completed = sum(1 for demand in self.env.vrptw.demands if demand.is_completed)
            if new_completed > completed_deliveries:
                completed_deliveries = new_completed
                truck_stats[agent]["deliveries"] += 1
                print(f"  âœ… ë°°ì†¡ ì™„ë£Œ! ì´ ì§„í–‰ë¥ : {completed_deliveries}/50 ({completed_deliveries/50*100:.1f}%)")
            
            # ë³´ìƒ ì¶œë ¥
            if reward != 0:
                print(f"  ğŸ’° Reward: {reward:+.1f}")
            
            if done:
                print(f"  ğŸ {agent} ì™„ë£Œ!")
                break
            
            # ì§„í–‰ ìƒí™© ì²´í¬
            if episode_step % 20 == 0:
                print(f"\nğŸ“Š Progress Check (Step {episode_step}):")
                print(f"   ğŸ“¦ Completed Deliveries: {completed_deliveries}/50")
                print(f"   â° Average Time: {np.mean([truck.current_time for truck in self.env.vrptw.delivery_trucks]):.1f}min")
                print("-" * 70)
            
            # ë„ˆë¬´ ê¸´ ì—í”¼ì†Œë“œ ë°©ì§€
            if episode_step > 1000:
                print("  â° ìµœëŒ€ ìŠ¤í… ìˆ˜ ë„ë‹¬ë¡œ ì—í”¼ì†Œë“œ ì¢…ë£Œ")
                break
        
        # ìµœì¢… ê²°ê³¼
        print("\n" + "="*70)
        print("ğŸ Episode Results:")
        print(f"ğŸ“Š Total Steps: {episode_step}")
        print(f"ğŸ“¦ Completed Deliveries: {completed_deliveries}/50")
        print(f"ğŸ“ˆ Success Rate: {completed_deliveries/50*100:.1f}%")
        print(f"â° Total Time: {max(truck.current_time for truck in self.env.vrptw.delivery_trucks):.1f} minutes")
        
        # íŠ¸ëŸ­ë³„ í†µê³„
        print("\nğŸšš Truck Performance:")
        for agent, stats in truck_stats.items():
            agent_idx = self.env.agent_name_idx_mapping[agent]
            truck = self.env.vrptw.delivery_trucks[agent_idx]
            print(f"  {agent}: {stats['deliveries']} deliveries, "
                  f"Final location: {truck.location}, "
                  f"Final time: {truck.current_time:.1f}min")
        
        print("="*70)
        
        return completed_deliveries, episode_step

# ì‹¤í–‰
if __name__ == "__main__":
    visualizer = VRPTWVisualizer(vis_env)
    
    # ì—¬ëŸ¬ ì—í”¼ì†Œë“œ ì‹¤í–‰ (ì›í•œë‹¤ë©´)
    num_episodes = 3
    total_deliveries = 0
    total_steps = 0
    
    for episode in range(num_episodes):
        print(f"\nğŸ¬ Starting Episode {episode + 1}/{num_episodes}")
        deliveries, steps = visualizer.run_episode()
        total_deliveries += deliveries
        total_steps += steps
        
        if episode < num_episodes - 1:
            input("\nâ¸ï¸  Press Enter to continue to next episode...")
    
    # ì „ì²´ ê²°ê³¼ ìš”ì•½
    if num_episodes > 1:
        print(f"\nğŸ‰ Overall Performance Summary:")
        print(f"ğŸ“Š Average Deliveries: {total_deliveries/num_episodes:.1f}/50")
        print(f"ğŸ“ˆ Average Success Rate: {total_deliveries/(num_episodes*50)*100:.1f}%")
        print(f"ğŸ¯ Average Steps: {total_steps/num_episodes:.0f}")
    
    vis_env.close()
    print("\nğŸ‰ VRPTW Visualization completed!")