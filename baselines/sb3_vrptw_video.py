import os
import numpy as np
import json
import gymnasium as gym
import random

import sumo_gym
from sumo_gym.utils.fmp_utils import (
    Vertex,
    Edge,
    Demand,
    DeliveryTruck,
    TimeManager,
    generate_random_time_windows,
    get_available_deliveries,
)
from DQN.dqn import (
    QNetwork,
    ReplayBuffer,
    run_target_update,
)
from statistics import mean

suffix = "-vrptw.json"

from sumo_gym.envs.fmp import VRPTWEnv

# ğŸ¬ ë…¹í™”ìš© í™˜ê²½ ìƒì„± (GUI ì¼œê¸°)
video_env = VRPTWEnv(
    mode="sumo_config",
    verbose=1,
    sumo_config_path="../assets/data/cosmos/cosmos.sumocfg",
    net_xml_file_path="../assets/data/cosmos/cosmos.net.xml",
    demand_xml_file_path="../assets/data/cosmos/cosmos.rou.xml",
    additional_xml_file_path="../assets/data/cosmos/cosmos.cs.add.xml",
    render_env=True,  # ğŸ¥ GUI ì¼œê¸° (ë…¹í™”ìš©)
)

class VRPTWVideoDemo(object):
    def __init__(self, env):
        self.env = env
        
        # í•™ìŠµëœ ëª¨ë¸ ë„¤íŠ¸ì›Œí¬ ì¬ìƒì„±
        state_size = 105  # í˜„ì¬ì‹œê°„(1) + ìœ„ì¹˜(1) + ì ì¬ëŸ‰(1) + ë°°ì†¡ìƒíƒœ(50) + ì‹œê°„ì°½ì •ë³´(50Ã—2)
        action_size = 52  # í—ˆë¸Œ(1) + ë°°ì†¡ì§€(50) + ëŒ€ê¸°(1)
        
        self.q_network = QNetwork(state_size, action_size, 0.003)
        self.time_manager = TimeManager(max_episode_time=180)  # 3ì‹œê°„
        
        # í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¶ˆëŸ¬ì˜¤ê¸° (ìˆë‹¤ë©´)
        # self.load_trained_weights()
    
    def load_trained_weights(self):
        """í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¶ˆëŸ¬ì˜¤ê¸° (êµ¬í˜„ í•„ìš”)"""
        # ì˜ˆ: self.q_network.load_weights("vrptw_model.pth")
        pass
    
    def _get_state_vector(self, agent):
        """í˜„ì¬ ìƒíƒœë¥¼ ë²¡í„°ë¡œ ë³€í™˜"""
        agent_idx = self.env.agent_name_idx_mapping[agent]
        truck = self.env.vrptw.delivery_trucks[agent_idx]
        
        # ê¸°ë³¸ ìƒíƒœ ì •ë³´
        state = [
            truck.current_time / 180.0,  # ì •ê·œí™”ëœ í˜„ì¬ ì‹œê°„
            truck.location / 50.0,       # ì •ê·œí™”ëœ ìœ„ì¹˜
            truck.cargo_count / 5.0,     # ì •ê·œí™”ëœ ì ì¬ëŸ‰
        ]
        
        # ë°°ì†¡ ì™„ë£Œ ìƒíƒœ (50ê°œ)
        delivery_status = [1.0 if demand.is_completed else 0.0 
                          for demand in self.env.vrptw.demands]
        state.extend(delivery_status)
        
        # ì‹œê°„ì°½ ì •ë³´ (50ê°œ Ã— 2)
        for demand in self.env.vrptw.demands:
            state.extend([
                demand.earliest_time / 180.0,  # ì •ê·œí™”ëœ ì‹œì‘ ì‹œê°„
                demand.latest_time / 180.0,    # ì •ê·œí™”ëœ ì¢…ë£Œ ì‹œê°„
            ])
        
        return np.array(state, dtype=np.float32)

    def _get_available_actions(self, agent):
        """í˜„ì¬ ìƒíƒœì—ì„œ ê°€ëŠ¥í•œ ì•¡ì…˜ ëª©ë¡"""
        agent_idx = self.env.agent_name_idx_mapping[agent]
        truck = self.env.vrptw.delivery_trucks[agent_idx]
        available_actions = []
        
        # í—ˆë¸Œ ë³µê·€ (í•­ìƒ ê°€ëŠ¥)
        available_actions.append(0)
        
        # ë°°ì†¡ì§€ ì´ë™ (ì ì¬ëŸ‰ì´ ìˆê³  ì™„ë£Œë˜ì§€ ì•Šì€ ë°°ì†¡ì§€)
        if truck.cargo_count > 0:
            for i, demand in enumerate(self.env.vrptw.demands):
                if not demand.is_completed:
                    # ì‹œê°„ ì œì•½ í™•ì¸
                    travel_time = self.time_manager.calculate_travel_time(
                        self.env.vrptw.vertices, self.env.vrptw.edges,
                        truck.location, demand.destination
                    )
                    arrival_time = truck.current_time + travel_time
                    
                    if arrival_time <= self.time_manager.max_episode_time:
                        available_actions.append(i + 1)  # ë°°ì†¡ì§€ ì•¡ì…˜
        
        # ëŒ€ê¸° (í•­ìƒ ê°€ëŠ¥)
        available_actions.append(51)
        
        return available_actions

    def _select_smart_action(self, agent):
        """ìŠ¤ë§ˆíŠ¸í•œ ì•¡ì…˜ ì„ íƒ (í•™ìŠµëœ ëª¨ë¸ ë˜ëŠ” íœ´ë¦¬ìŠ¤í‹±)"""
        state = self._get_state_vector(agent)
        available_actions = self._get_available_actions(agent)
        
        if not available_actions:
            return 51  # ëŒ€ê¸°
        
        # í•™ìŠµëœ ëª¨ë¸ì´ ìˆë‹¤ë©´ ì‚¬ìš©
        try:
            q_values = self.q_network.compute_q_values(state)
            # ê°€ëŠ¥í•œ ì•¡ì…˜ ì¤‘ì—ì„œ ìµœê³  Qê°’ ì„ íƒ
            available_q_values = [(action, q_values[action]) for action in available_actions]
            return max(available_q_values, key=lambda x: x[1])[0]
        except:
            # ëª¨ë¸ì´ ì—†ê±°ë‚˜ ì˜¤ë¥˜ê°€ ìˆìœ¼ë©´ íœ´ë¦¬ìŠ¤í‹± ì‚¬ìš©
            return self._heuristic_action(agent, available_actions)
    
    def _heuristic_action(self, agent, available_actions):
        """íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ ì•¡ì…˜ ì„ íƒ"""
        agent_idx = self.env.agent_name_idx_mapping[agent]
        truck = self.env.vrptw.delivery_trucks[agent_idx]
        
        # ì‹œê°„ì´ ë§ì´ ì§€ë‚¬ìœ¼ë©´ í—ˆë¸Œë¡œ ë³µê·€
        if truck.current_time > 150:  # 2.5ì‹œê°„ ì´í›„
            return 0
        
        # ì ì¬ëŸ‰ì´ ìˆìœ¼ë©´ ê°€ì¥ ê°€ê¹Œìš´ ë°°ì†¡ì§€ë¡œ
        if truck.cargo_count > 0:
            delivery_actions = [a for a in available_actions if 1 <= a <= 50]
            if delivery_actions:
                # ê°€ì¥ ê°€ê¹Œìš´ ë°°ì†¡ì§€ ì„ íƒ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
                return min(delivery_actions)
        
        # ê¸°ë³¸ì ìœ¼ë¡œ í—ˆë¸Œë¡œ ë³µê·€
        return 0
    
    def create_demo_video(self, num_episodes=3):
        """í•™ìŠµëœ ëª¨ë¸ë¡œ ë°ëª¨ ì˜ìƒ ìƒì„±"""
        
        print("ğŸ¬" + "="*60)
        print("ğŸšš VRPTW Fleet Management - VIDEO DEMO ğŸšš")
        print("ğŸ¬" + "="*60)
        print("ğŸ“¹ SUMO GUIê°€ ì—´ë¦½ë‹ˆë‹¤. í™”ë©´ ë…¹í™”ë¥¼ ì‹œì‘í•˜ì„¸ìš”!")
        print("ğŸ® GUI ì¡°ì‘ë²•:")
        print("   - Space: ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘/ì¼ì‹œì •ì§€")
        print("   - +/-: ì†ë„ ì¡°ì ˆ")
        print("   - Ctrl+A: ìµœëŒ€ ì†ë„")
        print("   - View â†’ Vehicle â†’ Show As: ì°¨ëŸ‰ í‘œì‹œ ì„¤ì •")
        print("ğŸ¬" + "="*60)
        
        input("ğŸ“¹ í™”ë©´ ë…¹í™” ì¤€ë¹„ê°€ ë˜ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")
        
        for episode in range(num_episodes):
            print(f"\nğŸ¬ Episode {episode + 1}/{num_episodes} ì‹œì‘!")
            
            self.env.reset()
            episode_step = 0
            total_deliveries = 0
            total_distance = 0
            
            print(f"ğŸ¯ Episode {episode + 1} ëª©í‘œ:")
            print("   - 50ê°œ ë°°ì†¡ì§€ ì„œë¹„ìŠ¤")
            print("   - ì‹œê°„ì°½ ì œì•½ ì¤€ìˆ˜") 
            print("   - íš¨ìœ¨ì ì¸ ê²½ë¡œ ì„ íƒ")
            print("   - 5ëŒ€ íŠ¸ëŸ­ í˜‘ë ¥ ìš´í–‰")
            
            for agent in self.env.agent_iter():
                agent_idx = self.env.agent_name_idx_mapping[agent]
                truck = self.env.vrptw.delivery_trucks[agent_idx]
                
                # ìŠ¤ë§ˆíŠ¸í•œ ì•¡ì…˜ ì„ íƒ
                action = self._select_smart_action(agent)
                
                # ì•¡ì…˜ ì´ë¦„ ë§¤í•‘
                if action == 0:
                    action_name = "ğŸ  HUB"
                elif 1 <= action <= 50:
                    action_name = f"ğŸ“¦ DELIVERY_{action}"
                else:
                    action_name = "â³ WAIT"
                
                print(f"   ğŸšš {agent} (Time: {truck.current_time:.1f}, "
                      f"Location: {truck.location}, Cargo: {truck.cargo_count}): {action_name}")
                
                # í™˜ê²½ì—ì„œ ì•¡ì…˜ ì‹¤í–‰
                observation, reward, done, info = self.env.last()
                self.env.step(action)
                
                episode_step += 1
                
                # ë°°ì†¡ ì™„ë£Œ ì²´í¬
                completed_deliveries = sum(1 for demand in self.env.vrptw.demands if demand.is_completed)
                if completed_deliveries > total_deliveries:
                    total_deliveries = completed_deliveries
                    print(f"   âœ… ë°°ì†¡ ì™„ë£Œ! ì´ {total_deliveries}/50ê°œ")
                
                if done:
                    break
                
                # ë„ˆë¬´ ê¸´ ì—í”¼ì†Œë“œ ë°©ì§€
                if episode_step > 500:
                    print("   â° ì‹œê°„ ì´ˆê³¼ë¡œ ì—í”¼ì†Œë“œ ì¢…ë£Œ")
                    break
            
            print(f"âœ… Episode {episode + 1} ì™„ë£Œ!")
            print(f"ğŸ“Š Steps: {episode_step}, Deliveries: {total_deliveries}/50")
            print(f"ğŸ“ˆ Success Rate: {total_deliveries/50*100:.1f}%")
            
            if episode < num_episodes - 1:
                input("ğŸ¬ ë‹¤ìŒ ì—í”¼ì†Œë“œë¥¼ ìœ„í•´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")
        
        print("\nğŸ‰" + "="*60)
        print("ğŸ¬ VRPTW ë¹„ë””ì˜¤ ë°ëª¨ ì™„ë£Œ!")
        print("ğŸ“¹ í™”ë©´ ë…¹í™”ë¥¼ ì¤‘ì§€í•˜ê³  íŒŒì¼ì„ ì €ì¥í•˜ì„¸ìš”!")
        print("ğŸ‰" + "="*60)

# ì‹¤í–‰
if __name__ == "__main__":
    print("ğŸ¬ VRPTW Fleet Management Video Demo")
    print("ğŸ’¡ ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” í•™ìŠµ ì™„ë£Œ í›„ ì‚¬ìš©í•˜ì„¸ìš”!")
    
    demo = VRPTWVideoDemo(video_env)
    demo.create_demo_video(num_episodes=3)
    
    video_env.close()
    print("ğŸŠ Demo ì™„ë£Œ!")