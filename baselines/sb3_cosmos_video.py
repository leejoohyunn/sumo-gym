import os
import numpy as np
import json
import gymnasium as gym
import random

import sumo_gym
from sumo_gym.utils.fmp_utils import (
    Vertex,
    Edge,
    Demand,
    ChargingStation,
    ElectricVehicles,
    get_dist_of_demands,
    get_dist_to_charging_stations,
    get_dist_to_finish_demands,
    get_safe_indicator,
)
from DQN.dqn import (
    QNetwork,
    LowerQNetwork_ChargingStation,
    LowerQNetwork_Demand,
    ReplayBuffer,
    run_target_update,
)
from statistics import mean

suffix = "-cosmos.json"

from sumo_gym.envs.fmp import FMPEnv

# ğŸ¬ ë…¹í™”ìš© í™˜ê²½ ìƒì„± (GUI ì¼œê¸°)
video_env = FMPEnv(
    mode="sumo_config",
    verbose=1,
    sumo_config_path="assets/data/cosmos/cosmos.sumocfg",
    net_xml_file_path="assets/data/cosmos/cosmos.net.xml",
    demand_xml_file_path="assets/data/cosmos/cosmos.rou.xml",
    additional_xml_file_path="assets/data/cosmos/cosmos.cs.add.xml",
    render_env=True,  # ğŸ¥ GUI ì¼œê¸° (ë…¹í™”ìš©)
)

class VideoMADQN(object):
    def __init__(self, env):
        self.env = env
        
        # í•™ìŠµëœ ëª¨ë¸ ë„¤íŠ¸ì›Œí¬ ì¬ìƒì„±
        self.q_principal_upper = {
            agent: QNetwork(
                1,
                env.action_space(agent).n,
                0.003,
            )
            for agent in self.env.agents
        }
        
        self.q_principal_lower_demand = LowerQNetwork_Demand(
            env.action_space_lower_demand().n_demand,
            env.action_space_lower_demand().n_demand,
            0.003,
        )
        
        self.q_principal_lower_cs = LowerQNetwork_ChargingStation(
            env.action_space_lower_cs().n_cs,
            env.action_space_lower_cs().n_cs,
            0.003,
        )
    
    def create_demo_video(self, num_episodes=3):
        """í•™ìŠµëœ ëª¨ë¸ë¡œ ë°ëª¨ ì˜ìƒ ìƒì„±"""
        
        print("ğŸ¬" + "="*60)
        print("ğŸš— COSMOS Fleet Management - VIDEO DEMO ğŸš—")
        print("ğŸ¬" + "="*60)
        print("ğŸ“¹ SUMO GUIê°€ ì—´ë¦½ë‹ˆë‹¤. í™”ë©´ ë…¹í™”ë¥¼ ì‹œì‘í•˜ì„¸ìš”!")
        print("ğŸ® GUI ì¡°ì‘ë²•:")
        print("   - Space: ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘/ì¼ì‹œì •ì§€")
        print("   - +/-: ì†ë„ ì¡°ì ˆ")
        print("   - Ctrl+A: ìµœëŒ€ ì†ë„")
        print("   - View â†’ Vehicle â†’ Show As: ì°¨ëŸ‰ í‘œì‹œ ì„¤ì •")
        print("   - View â†’ Persons â†’ Show As: ìŠ¹ê° í‘œì‹œ ì„¤ì •")
        print("ğŸ¬" + "="*60)
        
        input("ğŸ“¹ í™”ë©´ ë…¹í™” ì¤€ë¹„ê°€ ë˜ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")
        
        for episode in range(num_episodes):
            print(f"\nğŸ¬ Episode {episode + 1}/{num_episodes} ì‹œì‘!")
            
            self.env.reset()
            episode_step = 0
            total_reward = 0
            
            print(f"ğŸ¯ Episode {episode + 1} ëª©í‘œ:")
            print("   - ìŠ¹ê° í”½ì—… ì„œë¹„ìŠ¤")
            print("   - íš¨ìœ¨ì ì¸ ì¶©ì „ ê´€ë¦¬") 
            print("   - ìµœì  ê²½ë¡œ ì„ íƒ")
            
            for agent in self.env.agent_iter():
                upper_last, lower_last = self.env.last()
                observation, reward, done, info = upper_last
                
                # í•™ìŠµëœ ì •ì±… ì‚¬ìš© (ë˜ëŠ” ìŠ¤ë§ˆíŠ¸í•œ íœ´ë¦¬ìŠ¤í‹±)
                if observation == 3:
                    upper_action = 2  # ëŒ€ê¸°
                    action_name = "ğŸ›‘ WAIT"
                else:
                    # í•™ìŠµëœ ë„¤íŠ¸ì›Œí¬ ì‚¬ìš©
                    upper_action = self.q_principal_upper[agent].compute_argmax_q(observation)
                    action_names = ["ğŸš– PICKUP", "ğŸ”‹ CHARGE", "ğŸ›‘ WAIT"]
                    action_name = action_names[upper_action]
                
                # í•˜ìœ„ ë ˆë²¨ ì•¡ì…˜
                lower_observation, _, _, _ = lower_last
                if upper_action == 1:  # ì¶©ì „
                    lower_action = self.q_principal_lower_cs.compute_argmax_q(lower_observation[1])
                    print(f"   ğŸ¤– {agent}: {action_name} at Station {lower_action}")
                elif upper_action == 0:  # ìŠ¹ê° í”½ì—…
                    lower_action = self.q_principal_lower_demand.compute_argmax_q(lower_observation[0])
                    print(f"   ğŸ¤– {agent}: {action_name} Passenger {lower_action}")
                else:
                    lower_action = 0
                    print(f"   ğŸ¤– {agent}: {action_name}")
                
                self.env.step((upper_action, lower_action))
                total_reward += reward
                episode_step += 1
                
                if done:
                    break
            
            print(f"âœ… Episode {episode + 1} ì™„ë£Œ!")
            print(f"ğŸ“Š Steps: {episode_step}, Reward: {total_reward:.2f}")
            
            if episode < num_episodes - 1:
                input("ğŸ¬ ë‹¤ìŒ ì—í”¼ì†Œë“œë¥¼ ìœ„í•´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")
        
        print("\nğŸ‰" + "="*60)
        print("ğŸ¬ ë¹„ë””ì˜¤ ë°ëª¨ ì™„ë£Œ!")
        print("ğŸ“¹ í™”ë©´ ë…¹í™”ë¥¼ ì¤‘ì§€í•˜ê³  íŒŒì¼ì„ ì €ì¥í•˜ì„¸ìš”!")
        print("ğŸ‰" + "="*60)

# ì‹¤í–‰
if __name__ == "__main__":
    print("ğŸ¬ COSMOS Fleet Management Video Demo")
    print("ğŸ’¡ ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” í•™ìŠµ ì™„ë£Œ í›„ ì‚¬ìš©í•˜ì„¸ìš”!")
    
    demo = VideoMADQN(video_env)
    demo.create_demo_video(num_episodes=3)
    
    video_env.close()
    print("ğŸŠ Demo ì™„ë£Œ!")