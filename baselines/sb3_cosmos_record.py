import os
import numpy as np
import json
import gymnasium as gym
import random

import sumo_gym
from sumo_gym.utils.fmp_utils import (
    Vertex,
    Edge,
    Demand,
    ChargingStation,
    ElectricVehicles,
    get_dist_of_demands,
    get_dist_to_charging_stations,
    get_dist_to_finish_demands,
    get_safe_indicator,
)
from DQN.dqn import (
    QNetwork,
    LowerQNetwork_ChargingStation,
    LowerQNetwork_Demand,
    ReplayBuffer,
    run_target_update,
)
from statistics import mean

suffix = "-cosmos.json"

from sumo_gym.envs.fmp import FMPEnv

# ì‹œê°í™”ìš© í™˜ê²½ ìƒì„± (GUI ì¼œê¸°)
vis_env = FMPEnv(
    mode="sumo_config",
    verbose=1,
    sumo_config_path="assets/data/cosmos/cosmos.sumocfg",
    net_xml_file_path="assets/data/cosmos/cosmos.net.xml",
    demand_xml_file_path="assets/data/cosmos/cosmos.rou.xml",
    additional_xml_file_path="assets/data/cosmos/cosmos.cs.add.xml",
    render_env=True,  # GUI ì¼œê¸°
)

class MADQNVisualizer(object):
    def __init__(self, env):
        self.env = env
        
        # í•™ìŠµëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°ìš© ë„¤íŠ¸ì›Œí¬ ìƒì„±
        self.q_principal_upper = {
            agent: QNetwork(
                1,
                env.action_space(agent).n,
                0.003,
            )
            for agent in self.env.agents
        }
        
        self.q_principal_lower_demand = LowerQNetwork_Demand(
            env.action_space_lower_demand().n_demand,
            env.action_space_lower_demand().n_demand,
            0.003,
        )
        
        self.q_principal_lower_cs = LowerQNetwork_ChargingStation(
            env.action_space_lower_cs().n_cs,
            env.action_space_lower_cs().n_cs,
            0.003,
        )
        
        # í•™ìŠµëœ ê°€ì¤‘ì¹˜ê°€ ìˆë‹¤ë©´ ë¶ˆëŸ¬ì˜¤ê¸° (ì„ íƒì‚¬í•­)
        # self.load_trained_weights()
    
    def load_trained_weights(self):
        """í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¶ˆëŸ¬ì˜¤ê¸° (êµ¬í˜„ í•„ìš”)"""
        pass
    
    def run_episode(self):
        """í•™ìŠµëœ ëª¨ë¸ë¡œ 1 ì—í”¼ì†Œë“œ ì‹¤í–‰"""
        print("="*60)
        print("ğŸš— COSMOS Fleet Management Visualization ğŸš—")
        print("="*60)
        print("ğŸ“º SUMO GUI Controls:")
        print("   - Space: Start/Pause simulation")
        print("   - +/-: Speed up/down")
        print("   - Ctrl+A: Maximum speed")
        print("="*60)
        
        self.env.reset()
        episode_step = 0
        total_reward = 0
        
        for agent in self.env.agent_iter():
            upper_last, lower_last = self.env.last()
            observation, reward, done, info = upper_last
            
            print(f"Step {episode_step}: Agent {agent}")
            print(f"  Observation: {observation}, Reward: {reward}")
            
            # ìƒìœ„ ë ˆë²¨ ì•¡ì…˜ ê²°ì •
            if observation == 3:
                upper_action = 2  # ëŒ€ê¸°
                action_name = "WAIT"
            else:
                # í•™ìŠµëœ ë„¤íŠ¸ì›Œí¬ ì‚¬ìš© (ë˜ëŠ” ëœë¤)
                upper_action = self.q_principal_upper[agent].compute_argmax_q(observation)
                action_name = ["PICKUP", "CHARGE", "WAIT"][upper_action]
            
            # í•˜ìœ„ ë ˆë²¨ ì•¡ì…˜ ê²°ì •
            lower_observation, _, _, _ = lower_last
            if upper_action == 1:  # ì¶©ì „
                lower_action = self.q_principal_lower_cs.compute_argmax_q(lower_observation[1])
                print(f"  Action: {action_name} at charging station {lower_action}")
            elif upper_action == 0:  # ìŠ¹ê° í”½ì—…
                lower_action = self.q_principal_lower_demand.compute_argmax_q(lower_observation[0])
                print(f"  Action: {action_name} passenger {lower_action}")
            else:
                lower_action = 0
                print(f"  Action: {action_name}")
            
            self.env.step((upper_action, lower_action))
            total_reward += reward
            episode_step += 1
            
            if done:
                break
        
        print("="*60)
        print(f"ğŸ Episode completed!")
        print(f"ğŸ“Š Total steps: {episode_step}")
        print(f"ğŸ¯ Total reward: {total_reward}")
        print("="*60)
        
        return total_reward

# ì‹¤í–‰
if __name__ == "__main__":
    visualizer = MADQNVisualizer(vis_env)
    
    # ì—¬ëŸ¬ ì—í”¼ì†Œë“œ ì‹¤í–‰ (ì›í•œë‹¤ë©´)
    num_episodes = 3
    
    for episode in range(num_episodes):
        print(f"\nğŸ¬ Starting Episode {episode + 1}/{num_episodes}")
        reward = visualizer.run_episode()
        
        if episode < num_episodes - 1:
            input("Press Enter to continue to next episode...")
    
    vis_env.close()
    print("\nğŸ‰ Visualization completed!")